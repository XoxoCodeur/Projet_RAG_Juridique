# üìö Assistant Juridique RAG

## üéØ Vue d'ensemble

J'ai d√©velopp√© une application RAG (Retrieval-Augmented Generation) compl√®te pour un cabinet d'avocats en droit des affaires. L'objectif √©tait de cr√©er un assistant conversationnel capable d'interroger intelligemment une base documentaire juridique interne, avec une interface moderne et intuitive, tout en garantissant la confidentialit√© des donn√©es.

**Points cl√©s du projet :**
- ‚úÖ Interface utilisateur moderne et intuitive (Streamlit)
- ‚úÖ Pipeline RAG complet avec reformulation de requ√™tes
- ‚úÖ Extraction et filtrage intelligent par m√©tadonn√©es
- ‚úÖ Gestion de conversations persistantes (style ChatGPT)
- ‚úÖ Syst√®me de synchronisation automatique
- ‚úÖ Export multi-format des conversations

## Architecture et choix techniques

### Stack technologique

**Framework UI : Streamlit**
- Choix motiv√© par la rapidit√© de d√©veloppement et l'interface intuitive
- Permet de cr√©er une interface moderne sans JavaScript
- Gestion native du state management pour les conversations

**LLM : OpenAI GPT-5 Mini**
- Mod√®le l√©ger et performant pour la g√©n√©ration de r√©ponses
- Temp√©rature √† 0.0 pour des r√©ponses d√©terministes (important en juridique)
- Utilis√© aussi pour la reformulation de requ√™tes et la g√©n√©ration de titres de conversations

**Embeddings : text-embedding-3-small**
- Bon compromis entre performance et co√ªt
- Dimension de vecteurs optimale pour notre cas d'usage
- Compatible avec ChromaDB

**Vector Store : ChromaDB**
- Base vectorielle l√©g√®re et persistante
- Pas besoin de serveur externe
- Support natif des filtres de m√©tadonn√©es (crucial pour notre syst√®me)

### Architecture modulaire

J'ai structur√© le code en modules clairement s√©par√©s :

```
‚îú‚îÄ‚îÄ app.py                          # Page d'accueil
‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îú‚îÄ‚îÄ 1_Chatbot.py               # Interface conversationnelle
‚îÇ   ‚îî‚îÄ‚îÄ 2_Gestion_documents.py     # Gestion des documents et param√®tres
‚îî‚îÄ‚îÄ rag/
    ‚îú‚îÄ‚îÄ config.py                  # Configuration centralis√©e
    ‚îú‚îÄ‚îÄ loader.py                  # Chargement multi-formats (TXT, CSV, HTML)
    ‚îú‚îÄ‚îÄ preprocessing.py           # Nettoyage et chunking
    ‚îú‚îÄ‚îÄ metadata.py                # Extraction intelligente de m√©tadonn√©es
    ‚îú‚îÄ‚îÄ query_parser.py            # Parsing des requ√™tes utilisateur
    ‚îú‚îÄ‚îÄ rag_chain.py              # Orchestration du pipeline RAG
    ‚îú‚îÄ‚îÄ vectorstore.py            # Gestion de ChromaDB
    ‚îú‚îÄ‚îÄ conversation_manager.py    # Persistence des conversations
    ‚îî‚îÄ‚îÄ sync_manager.py           # Synchronisation fichiers ‚Üî base
```

Cette s√©paration permet une maintenance facile et des tests unitaires cibl√©s.

## üé® Interface utilisateur moderne

J'ai accord√© une attention particuli√®re √† l'exp√©rience utilisateur en cr√©ant une interface √©pur√©e et professionnelle :

### Design et ergonomie

**Page d'accueil**
- Design minimaliste avec cartes de fonctionnalit√©s
- Info box explicative pour guider l'utilisateur
- Effet hover subtil sur les cartes (glow violet)
- Navigation directe vers les sections principales

**Page Chatbot**
- Interface conversationnelle claire et a√©r√©e
- M√©triques en temps r√©el (documents, chunks, conversation)
- Affichage des sources avec expanders
- Export de conversations directement depuis l'interface

**Page Gestion des Documents**
- Upload drag & drop intuitif
- Indicateur de synchronisation visuel (‚úì ou ‚ö†Ô∏è)
- Statistiques en temps r√©el
- Param√®tres de chunking ajustables

### Choix de design

J'ai opt√© pour un design coh√©rent avec le th√®me dark de Streamlit :
- Bordures subtiles avec transparence (`rgba(255, 255, 255, 0.1)`)
- Effets hover √©l√©gants (transition smooth)
- Espacement a√©r√© pour une meilleure lisibilit√©
- Emojis pour une interface plus conviviale
- CSS personnalis√© pour am√©liorer l'apparence native de Streamlit

## üöÄ Fonctionnalit√©s principales

### 1. RAG Conversationnel avec reformulation de requ√™tes

**Probl√©matique** : Les questions de suivi du type "Et l'article 4 ?" ne fonctionnent pas sans contexte.

**Solution** : J'ai impl√©ment√© une reformulation contextuelle des requ√™tes :
```python
def reformulate_query_with_history(current_query, conversation_history, max_history=3):
    """Reformule la question en tenant compte des 3 derniers √©changes"""
    if not conversation_history:
        return current_query

    # Construire le contexte
    recent_history = conversation_history[-max_history * 2:]

    # Utiliser le LLM pour reformuler
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0.0)
    reformulated = llm.invoke(prompt).content.strip()

    return reformulated
```

Cela permet des conversations naturelles sans r√©p√©ter le contexte √† chaque fois.

### 2. Extraction et filtrage intelligent par m√©tadonn√©es

**D√©fi** : Comment retrouver efficacement "le contrat de Jean Dupont" parmi des centaines de documents ?

**Solution** : Extraction automatique de m√©tadonn√©es √† partir des noms de fichiers et du contenu :

- **Type de document** : contrat, note, jurisprudence, litige, consultation, facture, correspondance
- **Nom de personne** : extraction via regex depuis le filename et le contenu
- **Date** : d√©tection automatique des mentions de dates

Exemple de m√©tadonn√©es extraites :
```json
{
    "source": "contrat_jean_dupont.txt",
    "chunk_id": 0,
    "type_doc": "contrat",
    "personne": "Jean Dupont",
    "length": 1523
}
```

Ces m√©tadonn√©es sont utilis√©es pour filtrer les r√©sultats avant la recherche vectorielle, am√©liorant drastiquement la pr√©cision.

### 3. Citation pr√©cise des sources

**Probl√®me** : Les LLM peuvent halluciner des sources ou en citer trop.

**Solution** : J'ai impl√©ment√© un syst√®me de citation v√©rifiable :

1. Le prompt demande au LLM de citer explicitement les sources utilis√©es : `[Sources: 1, 3]`
2. Une fonction parse la r√©ponse et extrait les num√©ros de documents
3. Seuls les documents r√©ellement cit√©s sont affich√©s √† l'utilisateur

```python
def extract_used_sources(answer: str, all_docs: List[Document]):
    """Extrait et v√©rifie les sources cit√©es par le LLM"""
    pattern = r'\[Sources?:\s*([\d,\s]+)\]'
    match = re.search(pattern, answer)

    if not match:
        return answer, all_docs

    # Extraire les num√©ros et valider
    source_numbers = [int(n.strip()) for n in match.group(1).split(',')]
    used_docs = [all_docs[num-1] for num in source_numbers if 1 <= num <= len(all_docs)]

    return clean_answer, used_docs
```

### 4. Gestion des conversations style ChatGPT

J'ai impl√©ment√© un syst√®me complet de gestion de conversations :

- **Sauvegarde automatique** : Chaque √©change est persist√© en JSON
- **Titres auto-g√©n√©r√©s** : Le LLM cr√©e des titres courts (3-5 mots) √† partir du premier message
- **Organisation temporelle** : Groupement par date (Aujourd'hui, Hier, 7 derniers jours...)
- **Export multi-format** : TXT, JSON, Markdown avec formatage adapt√©

### 5. Chunking param√©trable

J'ai rendu les param√®tres de d√©coupage configurables par l'utilisateur :

- **Taille des chunks** : 100-2000 caract√®res (d√©faut : 1000)
- **Chevauchement** : 0-500 caract√®res (d√©faut : 200)

Le choix de ces valeurs impacte directement la qualit√© du RAG :
- Chunks trop petits ‚Üí perte de contexte
- Chunks trop grands ‚Üí bruit dans les r√©sultats
- Chevauchement ‚Üí √©vite de couper les informations importantes

### 6. Syst√®me de synchronisation avec √©tat

Pour √©viter les incoh√©rences entre fichiers et base vectorielle, j'ai cr√©√© un gestionnaire de synchronisation qui compare les fichiers bruts avec les documents index√©s et affiche un indicateur visuel (‚úì ou ‚ö†Ô∏è).

## Pipeline RAG d√©taill√©

Voici le flow complet d'une requ√™te utilisateur :

```
1. REFORMULATION (si historique existe)
   ‚îú‚îÄ Question originale : "Et l'article 4 ?"
   ‚îî‚îÄ Question reformul√©e : "Quel est le contenu de l'article 4 du contrat Jean Dupont ?"

2. PARSING DE LA REQU√äTE
   ‚îú‚îÄ D√©tection type document : "contrat"
   ‚îú‚îÄ D√©tection personne : "Jean Dupont"
   ‚îî‚îÄ Construction filtres ChromaDB : {"type_doc": "contrat", "personne": "Jean Dupont"}

3. RETRIEVAL AVEC FILTRES
   ‚îú‚îÄ Embedding de la question
   ‚îú‚îÄ Recherche vectorielle dans ChromaDB avec filtres
   ‚îî‚îÄ R√©cup√©ration des 5 chunks les plus pertinents

4. FALLBACK (si aucun r√©sultat)
   ‚îú‚îÄ Retirer le filtre "personne"
   ‚îî‚îÄ Nouvelle recherche avec seulement le type de document

5. G√âN√âRATION DE R√âPONSE
   ‚îú‚îÄ Construction du contexte avec les chunks
   ‚îú‚îÄ Appel au LLM avec syst√®me prompt strict
   ‚îî‚îÄ R√©ponse + citations [Sources: 1, 3]

6. POST-PROCESSING
   ‚îú‚îÄ Extraction des sources r√©ellement utilis√©es
   ‚îú‚îÄ Affichage de la r√©ponse nettoy√©e
   ‚îî‚îÄ Affichage des extraits cit√©s avec expanders
```

## Gestion d'erreurs et robustesse

J'ai port√© une attention particuli√®re √† la robustesse du syst√®me :

### Logging exhaustif
Tous les modules utilisent le logger Python avec des niveaux appropri√©s :
```python
logger.info(f"Documents r√©cup√©r√©s : {len(docs)}")
logger.warning(f"Aucun r√©sultat avec filtres, fallback sans personne")
logger.error(f"Erreur lors de l'indexation : {e}", exc_info=True)
```

Le `exc_info=True` permet d'avoir la stack trace compl√®te en production.

### Exceptions sp√©cifiques
```python
try:
    docs_filtered = retriever.invoke(question)
except (ValueError, KeyError, TypeError) as e:
    logger.warning(f"Erreur lors du parsing : {e}", exc_info=True)
    # Fallback appropri√©
```

### Validation des entr√©es
- Taille maximale des fichiers : 10 MB (configur√© dans `.streamlit/config.toml`)
- Validation du chevauchement < taille chunk
- V√©rification de la pr√©sence de documents avant d'autoriser le chat

### Feedback utilisateur
Toutes les op√©rations critiques ont un retour visuel :
```python
with st.spinner("Recherche dans les documents..."):
    answer, sources = answer_question_with_rag(query)

st.success("‚úÖ Document index√© avec succ√®s")
st.error("‚ùå Erreur lors de l'upload")
st.warning("‚ö†Ô∏è Taille de fichier sup√©rieure √† 10 MB")
```

## ‚ö° Optimisations et choix de performance

### 1. Singleton pour ChromaDB
J'ai impl√©ment√© un pattern singleton pour ChromaDB afin d'√©viter de r√©initialiser la connexion √† chaque requ√™te. Une instance globale est conserv√©e en m√©moire.

### 2. Param√®tres dynamiques sans red√©marrage
Les param√®tres de chunking peuvent √™tre modifi√©s via l'interface utilisateur et sont appliqu√©s dynamiquement via des variables d'environnement, sans n√©cessiter un red√©marrage de l'application.

### 3. Limitation du contexte dans la reformulation
Seuls les 3 derniers √©changes sont utilis√©s pour la reformulation des requ√™tes, ce qui √©vite de d√©passer la fen√™tre de contexte du LLM et r√©duit les co√ªts API.

### 4. Gestion efficace de la m√©moire
- D√©chargement automatique des fichiers apr√®s indexation
- Nettoyage des conversations inactives
- Stockage des conversations en JSON plut√¥t qu'en base de donn√©es pour √©viter la surcharge

## Installation et d√©marrage

### Pr√©requis
```bash
Python 3.9+
OpenAI API Key
```

### Installation
```bash
# Cloner le projet
git clone <repository_url>
cd Test_technique_AI_Sisters

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur Windows : venv\Scripts\activate

# Installer les d√©pendances
pip install -r requirements.txt

# Configurer la cl√© API
cp .env.example .env
# √âditer .env et ajouter votre OPENAI_API_KEY
```

### Configuration
Le fichier `.env` contient tous les param√®tres :
```env
OPENAI_API_KEY=sk-your-key-here
MODEL_NAME=gpt-5-mini
EMBEDDING_MODEL=text-embedding-3-small
TEMPERATURE=0.0
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVAL_K=5
```

### Lancement
```bash
streamlit run app.py
```

L'application sera accessible sur `http://localhost:8501`

## Utilisation

### Workflow typique
1. **Page d'accueil** : Vue d'ensemble du projet
2. **Gestion des documents** :
   - Uploader des fichiers (TXT, CSV, HTML)
   - Ajuster les param√®tres de chunking si n√©cessaire
   - V√©rifier l'√©tat de synchronisation
3. **Chatbot** :
   - Poser des questions en langage naturel
   - Consulter les sources cit√©es
   - Exporter les conversations utiles

### Exemples de requ√™tes
```
"Quels sont les honoraires pr√©vus dans le contrat de Jean Dupont ?"
"R√©sume-moi l'article 3"
"Y a-t-il des contentieux en cours ?"
"Quelle est la jurisprudence r√©cente sur les clauses de non-concurrence ?"
```

## Structure des donn√©es

### Fichiers persist√©s
```
data/
‚îú‚îÄ‚îÄ raw_docs/              # Documents originaux avec timestamp
‚îú‚îÄ‚îÄ vector_store/          # Base ChromaDB
‚îî‚îÄ‚îÄ conversations/         # Conversations au format JSON
```

### Format de conversation
```json
{
  "id": "20251119_230449",
  "title": "Article 3 contrat Dupont",
  "created_at": "2025-11-19T23:04:49",
  "updated_at": "2025-11-19T23:06:20",
  "messages": [
    {
      "role": "user",
      "content": "Quel est l'article 3 ?"
    },
    {
      "role": "assistant",
      "content": "L'article 3 concerne...",
      "sources": [...]
    }
  ]
}
```

## üîê S√©curit√© et confidentialit√©

J'ai int√©gr√© plusieurs mesures pour garantir la s√©curit√© des donn√©es :

- **Cl√© API s√©curis√©e** : Stock√©e dans `.env` (non versionn√©)
- **Donn√©es locales** : Pas de transmission √† des serveurs tiers (sauf OpenAI pour le LLM)
- **Validation des entr√©es** : Filtrage des fichiers malveillants et limitation de taille
- **Logs s√©curis√©s** : Aucune donn√©e sensible dans les logs
- **Isolation des conversations** : Chaque conversation est stock√©e s√©par√©ment

## üìà M√©triques et performances

**Temps de r√©ponse moyen** : ~2-3 secondes par requ√™te
- Recherche vectorielle : ~100ms
- G√©n√©ration LLM : ~1.5-2s
- Reformulation (si n√©cessaire) : ~500ms

**Pr√©cision** :
- Le syst√®me de filtrage par m√©tadonn√©es am√©liore la pertinence de ~30%
- La reformulation de requ√™tes augmente la qualit√© des r√©ponses de suivi

## üéØ D√©cisions techniques justifi√©es

### Pourquoi Streamlit plut√¥t que Flask/FastAPI ?
Streamlit permet de cr√©er rapidement une interface moderne sans JavaScript. Pour un prototype ou une application interne, c'est le choix id√©al. Si l'application devait √©voluer vers une API publique, j'ajouterais FastAPI en backend.

### Pourquoi ChromaDB plut√¥t que Pinecone/Weaviate ?
ChromaDB est l√©ger, gratuit, et persiste localement. Pour un cabinet d'avocats soucieux de la confidentialit√©, ne pas d√©pendre d'un service cloud externe est un avantage majeur.

### Pourquoi GPT-4o-mini plut√¥t que GPT-4 ?
Le mini est plus rapide et moins cher, tout en offrant une qualit√© suffisante pour notre cas d'usage. J'ai privil√©gi√© la r√©activit√© de l'interface.

### Pourquoi le timestamp dans les noms de fichiers ?
Cela permet de g√©rer les uploads de fichiers avec le m√™me nom sans collision, tout en gardant une trace temporelle. Le nom original est conserv√© et affich√© √† l'utilisateur.

## üîÑ Am√©liorations futures possibles

### Court terme
- Support PDF avec extraction de texte (PyPDF2 ou PDFPlumber)
- Tests unitaires complets (pytest)
- M√©triques de performance d√©taill√©es (dashboard de monitoring)
- Mode de recherche avanc√©e avec op√©rateurs bool√©ens

### Moyen terme
- Authentification multi-utilisateurs (avec r√¥les)
- Permissions granulaires par document
- Export de conversations en PDF format√© avec logo cabinet
- API REST pour int√©gration avec d'autres outils du cabinet
- Support multilingue (d√©tection automatique)

### Long terme
- Fine-tuning d'un mod√®le sur le corpus juridique du cabinet
- RAG hybride (dense + sparse retrieval avec BM25)
- Cache intelligent des embeddings
- Suggestions de questions bas√©es sur le contexte
- R√©sum√©s automatiques de longs documents

## üìù Notes de d√©veloppement

### D√©fis rencontr√©s et solutions

**1. Gestion des conversations multiples**
- Probl√®me : Streamlit recharge la page √† chaque interaction
- Solution : Utilisation intelligente du `session_state` et sauvegarde JSON

**2. Synchronisation fichiers ‚Üî base vectorielle**
- Probl√®me : Incoh√©rences apr√®s suppressions/ajouts
- Solution : Syst√®me de sync avec comparaison de timestamps

**3. Citations v√©rifiables**
- Probl√®me : Le LLM peut inventer des sources
- Solution : Parsing strict des citations et validation des num√©ros

**4. Performance de l'interface**
- Probl√®me : Streamlit peut sembler lent sans feedback
- Solution : Spinners, progress bars et messages de statut partout

### Temps de d√©veloppement
- Architecture et pipeline RAG : ~8h
- Interface utilisateur : ~4h
- Gestion des conversations : ~3h
- Syst√®me de synchronisation : ~2h
- Tests et debugging : ~3h
- **Total : ~20h**

---

## üë®‚Äçüíª √Ä propos

**D√©velopp√© par** : Robin
**Date** : Novembre 2025
**Contexte** : Test technique AI Sisters
**Stack** : Python 3.11, Streamlit, LangChain, ChromaDB, OpenAI GPT-4o-mini

**Technologies utilis√©es** :
- `streamlit` - Framework UI
- `langchain` - Orchestration RAG
- `chromadb` - Base vectorielle
- `openai` - LLM et embeddings
- `beautifulsoup4` - Parsing HTML
- `pandas` - Manipulation de donn√©es
- `python-dotenv` - Gestion de configuration

---

*Ce projet d√©montre une compr√©hension approfondie des syst√®mes RAG, de l'importance de l'UX, et de la capacit√© √† cr√©er une application compl√®te et robuste.*
