# ğŸ“š Test technique AI Sisters


**Points clÃ©s du projet :**
- âœ… Interface utilisateur moderne et intuitive (Streamlit)
- âœ… Pipeline RAG complet avec reformulation de requÃªtes
- âœ… Extraction et filtrage intelligent par mÃ©tadonnÃ©es
- âœ… Gestion de conversations persistantes (style ChatGPT)
- âœ… SystÃ¨me de synchronisation automatique
- âœ… Export multi-format des conversations

## Architecture et choix techniques

### Stack technologique

**Framework UI : Streamlit**

**LLM : OpenAI GPT-5 Mini**
- ModÃ¨le lÃ©ger et performant pour la gÃ©nÃ©ration de rÃ©ponses
- TempÃ©rature Ã  0.0 pour des rÃ©ponses dÃ©terministes (important en juridique)
- UtilisÃ© aussi pour la reformulation de requÃªtes et la gÃ©nÃ©ration de titres de conversations

**Embeddings : text-embedding-3-small**
- Bon compromis entre performance et coÃ»t
- Dimension de vecteurs optimale pour notre cas d'usage
- Compatible avec ChromaDB

**Vector Store : ChromaDB**
- Base vectorielle lÃ©gÃ¨re et persistante
- Pas besoin de serveur externe
- Support natif des filtres de mÃ©tadonnÃ©es (crucial pour notre systÃ¨me)

### Architecture modulaire

J'ai structurÃ© le code en modules clairement sÃ©parÃ©s :

```
â”œâ”€â”€ app.py                          # Page d'accueil
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 1_Chatbot.py               # Interface conversationnelle
â”‚   â””â”€â”€ 2_Gestion_documents.py     # Gestion des documents et paramÃ¨tres
â””â”€â”€ rag/
    â”œâ”€â”€ config.py                  # Configuration centralisÃ©e
    â”œâ”€â”€ loader.py                  # Chargement multi-formats (TXT, CSV, HTML)
    â”œâ”€â”€ preprocessing.py           # Nettoyage et chunking
    â”œâ”€â”€ metadata.py                # Extraction intelligente de mÃ©tadonnÃ©es
    â”œâ”€â”€ query_parser.py            # Parsing des requÃªtes utilisateur
    â”œâ”€â”€ rag_chain.py              # Orchestration du pipeline RAG
    â”œâ”€â”€ vectorstore.py            # Gestion de ChromaDB
    â”œâ”€â”€ conversation_manager.py    # Persistence des conversations
    â””â”€â”€ sync_manager.py           # Synchronisation fichiers â†” base
```


## ğŸ¨ Interface utilisateur moderne

J'ai accordÃ© une attention particuliÃ¨re Ã  l'expÃ©rience utilisateur :

### Design et ergonomie

**Page d'accueil**
- Design minimaliste avec cartes de fonctionnalitÃ©s
- Info box explicative pour guider l'utilisateur
- Effet hover subtil sur les cartes (glow violet)
- Navigation directe vers les sections principales

**Page Chatbot**
- Interface conversationnelle claire et aÃ©rÃ©e
- MÃ©triques en temps rÃ©el (documents, chunks, conversation)
- Affichage des sources avec expanders
- Export de conversations directement depuis l'interface

**Page Gestion des Documents**
- Upload drag & drop intuitif
- Indicateur de synchronisation visuel (âœ“ ou âš ï¸)
- Statistiques en temps rÃ©el
- ParamÃ¨tres de chunking ajustables
- ParamÃ¨tre de synchronisation automatique 


## ğŸš€ FonctionnalitÃ©s principales

### 1. RAG Conversationnel avec reformulation de requÃªtes

**ProblÃ©matique** : Les questions de suivi du type "Et l'article 4 ?" ne fonctionnent pas sans contexte.

**Solution** : J'ai implÃ©mentÃ© une reformulation contextuelle des requÃªtes :
```python
def reformulate_query_with_history(current_query, conversation_history, max_history=3):
    """Reformule la question en tenant compte des 3 derniers Ã©changes"""
    if not conversation_history:
        return current_query

    # Construire le contexte
    recent_history = conversation_history[-max_history * 2:]

    # Utiliser le LLM pour reformuler
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0.0)
    reformulated = llm.invoke(prompt).content.strip()

    return reformulated
```

Cela permet des conversations naturelles sans rÃ©pÃ©ter le contexte Ã  chaque fois.

### 2. Extraction et filtrage intelligent par mÃ©tadonnÃ©es

**DÃ©fi** : Comment retrouver efficacement "le contrat de Jean Dupont" parmi des centaines de documents ?

**Solution** : Extraction automatique de mÃ©tadonnÃ©es Ã  partir des noms de fichiers et du contenu :

- **Type de document** : contrat, note, jurisprudence, litige, consultation, facture, correspondance
- **Nom de personne** : extraction via regex depuis le filename et le contenu
- **Date** : dÃ©tection automatique des mentions de dates

Exemple de mÃ©tadonnÃ©es extraites :
```json
{
    "source": "contrat_jean_dupont.txt",
    "chunk_id": 0,
    "type_doc": "contrat",
    "personne": "Jean Dupont",
    "length": 1523
}
```

Ces mÃ©tadonnÃ©es sont utilisÃ©es pour filtrer les rÃ©sultats avant la recherche vectorielle, amÃ©liorant drastiquement la prÃ©cision.

### 3. Citation prÃ©cise des sources

**ProblÃ¨me** : Les LLM peuvent halluciner des sources ou en citer trop.

**Solution** : J'ai implÃ©mentÃ© un systÃ¨me de citation vÃ©rifiable :

1. Le prompt demande au LLM de citer explicitement les sources utilisÃ©es : `[Sources: 1, 3]`
2. Une fonction parse la rÃ©ponse et extrait les numÃ©ros de documents
3. Seuls les documents rÃ©ellement citÃ©s sont affichÃ©s Ã  l'utilisateur

```python
def extract_used_sources(answer: str, all_docs: List[Document]):
    """Extrait et vÃ©rifie les sources citÃ©es par le LLM"""
    pattern = r'\[Sources?:\s*([\d,\s]+)\]'
    match = re.search(pattern, answer)

    if not match:
        return answer, all_docs

    # Extraire les numÃ©ros et valider
    source_numbers = [int(n.strip()) for n in match.group(1).split(',')]
    used_docs = [all_docs[num-1] for num in source_numbers if 1 <= num <= len(all_docs)]

    return clean_answer, used_docs
```

### 4. Gestion des conversations style ChatGPT

J'ai implÃ©mentÃ© un systÃ¨me complet de gestion de conversations :

- **Sauvegarde automatique** : Chaque Ã©change est persistÃ© en JSON
- **Titres auto-gÃ©nÃ©rÃ©s** : Le LLM crÃ©e des titres courts (3-5 mots) Ã  partir du premier message
- **Organisation temporelle** : Groupement par date (Aujourd'hui, Hier, 7 derniers jours...)
- **Export multi-format** : TXT, JSON, Markdown avec formatage adaptÃ©

### 5. Chunking paramÃ©trable

J'ai rendu les paramÃ¨tres de dÃ©coupage configurables par l'utilisateur :

- **Taille des chunks** : 100-2000 caractÃ¨res (dÃ©faut : 1000)
- **Chevauchement** : 0-500 caractÃ¨res (dÃ©faut : 200)

Le choix de ces valeurs impacte directement la qualitÃ© du RAG :
- Chunks trop petits â†’ perte de contexte
- Chunks trop grands â†’ bruit dans les rÃ©sultats
- Chevauchement â†’ Ã©vite de couper les informations importantes

### 6. SystÃ¨me de synchronisation avec Ã©tat

Pour Ã©viter les incohÃ©rences entre fichiers et base vectorielle, j'ai crÃ©Ã© un gestionnaire de synchronisation qui compare les fichiers bruts avec les documents indexÃ©s et affiche un indicateur visuel (âœ“ ou âš ï¸).

## Pipeline RAG dÃ©taillÃ©

Voici le flow complet d'une requÃªte utilisateur :

```
1. REFORMULATION (si historique existe)
   â”œâ”€ Question originale : "Et l'article 4 ?"
   â””â”€ Question reformulÃ©e : "Quel est le contenu de l'article 4 du contrat Jean Dupont ?"

2. PARSING DE LA REQUÃŠTE
   â”œâ”€ DÃ©tection type document : "contrat"
   â”œâ”€ DÃ©tection personne : "Jean Dupont"
   â””â”€ Construction filtres ChromaDB : {"type_doc": "contrat", "personne": "Jean Dupont"}

3. RETRIEVAL AVEC FILTRES
   â”œâ”€ Embedding de la question
   â”œâ”€ Recherche vectorielle dans ChromaDB avec filtres
   â””â”€ RÃ©cupÃ©ration des 5 chunks les plus pertinents

4. FALLBACK (si aucun rÃ©sultat)
   â”œâ”€ Retirer le filtre "personne"
   â””â”€ Nouvelle recherche avec seulement le type de document

5. GÃ‰NÃ‰RATION DE RÃ‰PONSE
   â”œâ”€ Construction du contexte avec les chunks
   â”œâ”€ Appel au LLM avec systÃ¨me prompt strict
   â””â”€ RÃ©ponse + citations [Sources: 1, 3]

6. POST-PROCESSING
   â”œâ”€ Extraction des sources rÃ©ellement utilisÃ©es
   â”œâ”€ Affichage de la rÃ©ponse nettoyÃ©e
   â””â”€ Affichage des extraits citÃ©s avec expanders
```

## Gestion d'erreurs et robustesse

J'ai portÃ© une attention particuliÃ¨re Ã  la robustesse du systÃ¨me :

### Logging exhaustif
Tous les modules utilisent le logger Python avec des niveaux appropriÃ©s :
```python
logger.info(f"Documents rÃ©cupÃ©rÃ©s : {len(docs)}")
logger.warning(f"Aucun rÃ©sultat avec filtres, fallback sans personne")
logger.error(f"Erreur lors de l'indexation : {e}", exc_info=True)
```

Le `exc_info=True` permet d'avoir la stack trace complÃ¨te en production.

### Validation des entrÃ©es
- Taille maximale des fichiers : 10 MB (configurÃ© dans `.streamlit/config.toml`)
- Validation du chevauchement < taille chunk
- VÃ©rification de la prÃ©sence de documents avant d'autoriser le chat

### Feedback utilisateur
Toutes les opÃ©rations critiques ont un retour visuel :
```python
with st.spinner("Recherche dans les documents..."):
    answer, sources = answer_question_with_rag(query)

st.success("âœ… Document indexÃ© avec succÃ¨s")
st.error("âŒ Erreur lors de l'upload")
st.warning("âš ï¸ Taille de fichier supÃ©rieure Ã  10 MB")
```

## âš¡ Optimisations et choix de performance

### 1. Singleton pour ChromaDB
J'ai implÃ©mentÃ© un pattern singleton pour ChromaDB afin d'Ã©viter de rÃ©initialiser la connexion Ã  chaque requÃªte. Une instance globale est conservÃ©e en mÃ©moire.

### 2. ParamÃ¨tres dynamiques sans redÃ©marrage
Les paramÃ¨tres de chunking peuvent Ãªtre modifiÃ©s via l'interface utilisateur et sont appliquÃ©s dynamiquement via des variables d'environnement, sans nÃ©cessiter un redÃ©marrage de l'application.

### 3. Limitation du contexte dans la reformulation
Seuls les 3 derniers Ã©changes sont utilisÃ©s pour la reformulation des requÃªtes, ce qui Ã©vite de dÃ©passer la fenÃªtre de contexte du LLM et rÃ©duit les coÃ»ts API.
Ce paramÃ¨tre est Ã  adapter en fonction des usages/besoins

### 4. Gestion efficace de la mÃ©moire
- DÃ©chargement automatique des fichiers aprÃ¨s indexation
- Nettoyage des conversations inactives
- Stockage des conversations en JSON plutÃ´t qu'en base de donnÃ©es pour Ã©viter la surcharge

## Installation et dÃ©marrage

### PrÃ©requis
```bash
Python 3.9+
OpenAI API Key
```

### Installation
```bash
# Cloner le projet
git clone <repository_url>
cd Test_technique_AI_Sisters

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur Windows : venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt

# Configurer la clÃ© API
cp .env.example .env
# Ã‰diter .env et ajouter votre OPENAI_API_KEY
```

### Configuration
Le fichier `.env` contient tous les paramÃ¨tres :
```env
OPENAI_API_KEY=sk-your-key-here
MODEL_NAME=gpt-5-mini
EMBEDDING_MODEL=text-embedding-3-small
TEMPERATURE=0.0
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVAL_K=5
```

### Lancement
```bash
streamlit run app.py
```

L'application sera accessible sur `http://localhost:8501`



## Structure des donnÃ©es

### Fichiers persistÃ©s
```
data/
â”œâ”€â”€ raw_docs/              # Documents originaux avec timestamp
â”œâ”€â”€ vector_store/          # Base ChromaDB
â””â”€â”€ conversations/         # Conversations au format JSON
```

### Format de conversation
```json
{
  "id": "20251119_230449",
  "title": "Article 3 contrat Dupont",
  "created_at": "2025-11-19T23:04:49",
  "updated_at": "2025-11-19T23:06:20",
  "messages": [
    {
      "role": "user",
      "content": "Quel est l'article 3 ?"
    },
    {
      "role": "assistant",
      "content": "L'article 3 concerne...",
      "sources": [...]
    }
  ]
}
```

## ğŸ” SÃ©curitÃ© et confidentialitÃ©

J'ai intÃ©grÃ© plusieurs mesures pour garantir la sÃ©curitÃ© des donnÃ©es :

- **ClÃ© API sÃ©curisÃ©e** : StockÃ©e dans `.env` (non versionnÃ©)
- **DonnÃ©es locales** : Pas de transmission Ã  des serveurs tiers (sauf OpenAI pour le LLM)
- **Logs sÃ©curisÃ©s** : Aucune donnÃ©e sensible dans les logs
- **Isolation des conversations** : Chaque conversation est stockÃ©e sÃ©parÃ©ment



## ğŸ‘¨â€ğŸ’» Ã€ propos

**DÃ©veloppÃ© par** : Robin Baret - robin.baret1@gmail.com - 06 51 26 00 76
**Date** : Novembre 2025
**Contexte** : Test technique AI Sisters
**Stack** : Python 3.11, Streamlit, ChromaDB, OpenAI

**Technologies utilisÃ©es** :
- `streamlit` - Framework UI
- `langchain` - Orchestration RAG
- `chromadb` - Base vectorielle
- `openai` - LLM et embeddings
- `beautifulsoup4` - Parsing HTML
- `pandas` - Manipulation de donnÃ©es
- `python-dotenv` - Gestion de configuration

---


